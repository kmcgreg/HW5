{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from sklearn import tree \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has 500 rows and 10000 columns \n"
     ]
    }
   ],
   "source": [
    "categories = ('comp.windows.x', 'rec.sport.hockey')\n",
    "\n",
    "easy_dataset = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "tf_vec = CountVectorizer (max_df=500, \n",
    "                          min_df=0, \n",
    "                          max_features =10000, \n",
    "                          ngram_range =(1,1), \n",
    "                          stop_words='english')\n",
    "\n",
    "tf_matrix = tf_vec.fit_transform(easy_dataset.data[:500])  #sparse matrix\n",
    "\n",
    "print (\"the data has {} rows and {} columns \".format(tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=tf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify data\n",
    "t=np.asarray(easy_dataset.target[:500])   # true labels\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(), t, random_state=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naiive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 125 points: 61\n"
     ]
    }
   ],
   "source": [
    "clf = NB()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 125 points: 62\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has 500 rows and 10000 columns \n"
     ]
    }
   ],
   "source": [
    "categories = ('rec.motorcycles', 'rec.autos')\n",
    "\n",
    "difficult_dataset = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "tf_vec = CountVectorizer (max_df=500, \n",
    "                          min_df=0, \n",
    "                          max_features =10000, \n",
    "                          ngram_range =(1,1), \n",
    "                          stop_words='english')\n",
    "\n",
    "tf_matrix = tf_vec.fit_transform(easy_dataset.data[:500])  #sparse matrix\n",
    "\n",
    "print (\"the data has {} rows and {} columns \".format(tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=tf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify data\n",
    "t=np.asarray(difficult_dataset.target[:500])   # true labels\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(), t, random_state=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naiive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 125 points: 62\n"
     ]
    }
   ],
   "source": [
    "clf = NB()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 125 points: 68\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "From the classification results we can say that it's easier to classfy easy dataset then difficult dataset, as the naming of each dataset implied. Additionally, the Naiive Base classifer did better on this data set than a Decision Tree (but this appears to be purely due to chance, as occasionally the opposite is true...I guess I'm saying that they are fairly equivalent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 125 points: 61\n"
     ]
    }
   ],
   "source": [
    "# Bagging\n",
    "bagging = BaggingClassifier(tree.DecisionTreeClassifier())\n",
    "y_pred = bagging.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 125 points: 62\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(NB(), algorithm=\"SAMME\", n_estimators=300)\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 125 points: 63\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=10, max_features='auto')\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "From the above results we can say that Bagging, AdaBoost, Random forest models have pretty close to the same score as Naiive Bayes and Decision tree models; with NB doing marginally better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has 2323 rows and 29847 columns \n"
     ]
    }
   ],
   "source": [
    "categories = ('comp.graphics', 'rec.autos', 'talk.politics.guns', 'soc.religion.christian')\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "tf_vec = CountVectorizer (max_df=500, \n",
    "                          min_df=0, \n",
    "                          max_features =30000, \n",
    "                          ngram_range =(1,1), \n",
    "                          stop_words='english')\n",
    "\n",
    "tf_matrix = tf_vec.fit_transform(dataset.data)  #sparse matrix\n",
    "\n",
    "print (\"the data has {} rows and {} columns \".format(tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=tf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify data\n",
    "t=np.asarray(dataset.target)   # true labels\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(), t, random_state=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naiive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 581 points: 125\n"
     ]
    }
   ],
   "source": [
    "clf = NB()\n",
    "\n",
    "y_pred = OneVsRestClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 581 points: 126\n"
     ]
    }
   ],
   "source": [
    "clf =  tree.DecisionTreeClassifier()\n",
    "\n",
    "y_pred = OneVsRestClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 581 points: 92\n"
     ]
    }
   ],
   "source": [
    "clf =  LinearSVC()\n",
    "\n",
    "y_pred = OneVsRestClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-vs-all classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naiive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 581 points: 71\n"
     ]
    }
   ],
   "source": [
    "clf = NB()\n",
    "\n",
    "y_pred = OneVsOneClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 581 points: 123\n"
     ]
    }
   ],
   "source": [
    "clf =  tree.DecisionTreeClassifier()\n",
    "\n",
    "y_pred = OneVsOneClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 581 points: 100\n"
     ]
    }
   ],
   "source": [
    "clf =  LinearSVC()\n",
    "\n",
    "y_pred = OneVsOneClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of {} points: {}\".format(xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance results\n",
    "1. one vs one Naiive Bayes\n",
    "2. one vs all SVC\n",
    "3. all vs all SVC\n",
    "4. one vs one Decision tree\n",
    "5. one vs one Naiive Bayes\n",
    "6. all vs all Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "How does the classifier process the weights of the data points to focus on misclassified data points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given (x1,y1)….(xn,yn) and initial weights of data points wi = 1/n, i=1…n\n",
    "\n",
    "Weights are updated according to the formula: wi = wi*exp⁡(cm.1(y≠fm(x)), i=1…n\n",
    "\n",
    "but don't take my word for it, that's just what hours of combing through stack overflow told me. \n",
    "\n",
    "my guess in human/low level of conceptual understanding is: the process uses an error calulation between predicted and observed values based on the test data, this weight is then used to increase attention to those points...but honestly, I would just trust the stack overflow version. Those people really seem to know what they're talking about. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
